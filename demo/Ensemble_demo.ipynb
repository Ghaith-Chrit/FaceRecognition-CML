{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request as urlreq\n",
    "\n",
    "from typing import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from kymatio.numpy import Scattering2D\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "import scipy.io as scio\n",
    "from face_frontalization import frontalize\n",
    "from face_frontalization import camera_calibration as calib\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 8\n",
    "testing_person = \"Adrien\"\n",
    "\n",
    "J = 3\n",
    "k_histogram = 100\n",
    "min_image_num = 10\n",
    "training_set_size = 10\n",
    "training_image_num = 5\n",
    "num_misclassified_to_show = 5\n",
    "face_image_target_size = (64, 64)\n",
    "\n",
    "base_folder = \"demo/dataset-easy\"\n",
    "haarcascade = \"model_checkpoints/haarcascade.xml\"\n",
    "haarcascade_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt2.xml\"\n",
    "LBFmodel = \"model_checkpoints/lbfmodel.yaml\"\n",
    "LBFmodel_url = \"https://github.com/kurnianggoro/GSOC2017/raw/master/data/lbfmodel.yaml\"\n",
    "\n",
    "frontalize_model_name = \"model_dlib\"\n",
    "frontalize_model_path = \"model_checkpoints/model3Ddlib.mat\"\n",
    "\n",
    "eye_mask_mat = \"eyemask\"\n",
    "eye_mask_mat_path = \"model_checkpoints/eyemask.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, url in zip([haarcascade, LBFmodel], [haarcascade_url, LBFmodel_url]):\n",
    "    if os.path.exists(filename):\n",
    "        print(\"File exists\")\n",
    "    else:\n",
    "        urlreq.urlretrieve(url, filename)\n",
    "        print(\"File downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for person in os.listdir(base_folder):\n",
    "    person_dir = os.path.join(base_folder, person)\n",
    "    if os.path.isdir(person_dir):\n",
    "        images = []\n",
    "        for img_name in os.listdir(person_dir):\n",
    "            img_path = os.path.join(person_dir, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "        if len(images) >= min_image_num:\n",
    "            data[person] = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "people_names = list(data.keys())\n",
    "\n",
    "for person, images in data.items():\n",
    "    train_images = []\n",
    "    test_images = []\n",
    "    random.seed(random_seed)\n",
    "    random.shuffle(images)\n",
    "\n",
    "    train_images.extend(images[:training_image_num])\n",
    "    test_images.extend(images[training_image_num:])\n",
    "    train_labels = [person] * training_image_num\n",
    "    test_labels = [person] * (len(images) - training_image_num)\n",
    "\n",
    "    random.seed(random_seed)\n",
    "    other_people = (\n",
    "        [p for p in people_names if p != person]\n",
    "        * 2\n",
    "        * (training_set_size - training_image_num)\n",
    "    )\n",
    "\n",
    "    for i, other_person in enumerate(other_people):\n",
    "        chosen_image = random.sample(data[other_person], 1)[0]\n",
    "        if i % 2 == 0:\n",
    "            train_images.append(chosen_image)\n",
    "            train_labels.append(\"Unknown\")\n",
    "        else:\n",
    "            test_images.append(chosen_image)\n",
    "            test_labels.append(\"Unknown\")\n",
    "\n",
    "    x_train.append(train_images)\n",
    "    x_test.append(test_images)\n",
    "    y_train.append(train_labels)\n",
    "    y_test.append(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_remove = [i for i, inner in enumerate(y_train) if testing_person not in inner]\n",
    "for i in i_to_remove:\n",
    "    del x_train[i]\n",
    "    del x_test[i]\n",
    "    del y_train[i]\n",
    "    del y_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scattering = Scattering2D(\n",
    "    J=J,\n",
    "    shape=(face_image_target_size),\n",
    ")\n",
    "\n",
    "face_detector = cv2.CascadeClassifier(haarcascade)\n",
    "landmark_detector = cv2.face.createFacemarkLBF()\n",
    "landmark_detector.loadModel(LBFmodel)\n",
    "\n",
    "sift = cv2.SIFT_create(contrastThreshold=0, edgeThreshold=sys.maxsize)\n",
    "bf = cv2.BFMatcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sift_features(image_list):\n",
    "    keypoints_list = []\n",
    "    descriptors_list = []\n",
    "\n",
    "    for img in image_list:\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "\n",
    "        keypoints_list.append(keypoints)\n",
    "        descriptors_list.append(descriptors)\n",
    "\n",
    "    return keypoints_list, descriptors_list\n",
    "\n",
    "\n",
    "def create_bow_histogram(descriptors, kmeans):\n",
    "    if descriptors is None or len(descriptors) == 0:\n",
    "        return np.zeros(kmeans.cluster_centers_.shape[0])\n",
    "\n",
    "    cluster_indices = kmeans.predict(descriptors)\n",
    "    hist, _ = np.histogram(\n",
    "        cluster_indices,\n",
    "        bins=np.arange(kmeans.n_clusters + 1),\n",
    "        range=(0, kmeans.n_clusters),\n",
    "    )\n",
    "\n",
    "    return hist\n",
    "\n",
    "\n",
    "def train_bovw_svm(descriptors_list, label_list):\n",
    "    all_descriptors = np.vstack(descriptors_list)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k_histogram)\n",
    "    kmeans.fit(all_descriptors)\n",
    "\n",
    "    histograms = []\n",
    "    for descriptors in descriptors_list:\n",
    "        hist = create_bow_histogram(descriptors, kmeans)\n",
    "        histograms.append(hist)\n",
    "\n",
    "    svm = SVC(kernel=\"linear\")\n",
    "    svm.fit(histograms, label_list)\n",
    "\n",
    "    return svm, kmeans\n",
    "\n",
    "\n",
    "def predict_with_svm(svm, kmeans, new_image):\n",
    "    _, new_descriptors = sift.detectAndCompute(new_image, None)\n",
    "    if new_descriptors is None or len(new_descriptors) == 0:\n",
    "        return None\n",
    "\n",
    "    new_hist = create_bow_histogram(new_descriptors, kmeans)\n",
    "    prediction = svm.predict([new_hist])\n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_face(img):\n",
    "    model3D = frontalize.ThreeD_Model(frontalize_model_path, frontalize_model_name)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    if len(faces) == 0:\n",
    "        raise RuntimeError(\"No faces detected.\")\n",
    "\n",
    "    main_face = np.array([max(faces, key=lambda rect: rect[2] * rect[3])])\n",
    "    retval, landmarks = landmark_detector.fit(gray, main_face)\n",
    "    if not retval or len(landmarks) == 0:\n",
    "        raise RuntimeError(\"Could not detect landmarks.\")\n",
    "\n",
    "    # OpenCV returns landmarks as a list, where each element is an array of shape (1, 68, 2).\n",
    "    lmarks = landmarks[0][0]\n",
    "    proj_matrix, _, _, _ = calib.estimate_camera(model3D, lmarks)\n",
    "\n",
    "    eyemask = np.asarray(scio.loadmat(eye_mask_mat_path)[eye_mask_mat])\n",
    "    frontal_raw, frontal_sym = frontalize.frontalize(\n",
    "        img, proj_matrix, model3D.ref_U, eyemask\n",
    "    )\n",
    "\n",
    "    return frontal_raw, frontal_sym\n",
    "\n",
    "\n",
    "def obtain_only_face(i, frontal_view):\n",
    "    faces = face_detector.detectMultiScale(\n",
    "        frontal_view, scaleFactor=1.1, minNeighbors=5\n",
    "    )\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        raise RuntimeError(f\"No faces detected (after frontalization) {i}.\")\n",
    "\n",
    "    main_face = np.array([max(faces, key=lambda rect: rect[2] * rect[3])])\n",
    "    _, landmarks = landmark_detector.fit(frontal_view, main_face)\n",
    "\n",
    "    lmarks = landmarks[0][0]\n",
    "    hull = cv2.convexHull(np.array(lmarks, dtype=np.int32))\n",
    "\n",
    "    min_x = min(lmarks, key=lambda p: p[0])[0]\n",
    "    max_x = max(lmarks, key=lambda p: p[0])[0]\n",
    "    min_y = min(lmarks, key=lambda p: p[1])[1]\n",
    "    max_y = max(lmarks, key=lambda p: p[1])[1]\n",
    "\n",
    "    mask = np.zeros((frontal_view.shape[0], frontal_view.shape[1]), dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, [hull], 255)\n",
    "\n",
    "    masked_face = frontal_view.copy()\n",
    "    if masked_face.dtype != np.uint8:\n",
    "        masked_face = np.uint8(np.clip(masked_face, 0, 255))\n",
    "\n",
    "    masked_face[mask == 0] = 0\n",
    "    masked_face = masked_face[\n",
    "        int(min_y) - 5 : int(max_y) + 5, int(min_x) - 5 : int(max_x) + 5\n",
    "    ]\n",
    "\n",
    "    masked_face = cv2.cvtColor(masked_face, cv2.COLOR_BGR2GRAY)\n",
    "    resized_face = cv2.resize(masked_face, face_image_target_size)\n",
    "    return resized_face\n",
    "\n",
    "\n",
    "def normalize_list(image_list, useSym):\n",
    "    frontalized_input = [normalize_face(x)[1 if useSym else 0] for x in image_list]\n",
    "\n",
    "    cropped_input = [obtain_only_face(i, x) for i, x in enumerate(image_list)]\n",
    "    frontalized_cropped_input = [\n",
    "        obtain_only_face(i, x) for i, x in enumerate(frontalized_input)\n",
    "    ]\n",
    "\n",
    "    return cropped_input, frontalized_cropped_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true_labels, predicted_labels):\n",
    "    precision = precision_score(\n",
    "        true_labels,\n",
    "        predicted_labels,\n",
    "        average=\"weighted\",\n",
    "        labels=np.unique(true_labels),\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    recall = recall_score(\n",
    "        true_labels,\n",
    "        predicted_labels,\n",
    "        average=\"weighted\",\n",
    "        labels=np.unique(true_labels),\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    f1 = f1_score(\n",
    "        true_labels,\n",
    "        predicted_labels,\n",
    "        average=\"weighted\",\n",
    "        labels=np.unique(true_labels),\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def track_misclassifications(test_images, true_labels, predicted_labels):\n",
    "    misclassified_images = []\n",
    "    misclassified_true_labels = []\n",
    "    misclassified_pred_labels = []\n",
    "\n",
    "    true_labels = np.array(true_labels)\n",
    "    misclassified_indices = np.where(predicted_labels != true_labels)[0]\n",
    "\n",
    "    for idx in misclassified_indices:\n",
    "        misclassified_images.append(test_images[idx])\n",
    "        misclassified_true_labels.append(true_labels[idx])\n",
    "        misclassified_pred_labels.append(predicted_labels[idx])\n",
    "\n",
    "    return misclassified_images, misclassified_true_labels, misclassified_pred_labels\n",
    "\n",
    "\n",
    "def visualize_misclassifications(\n",
    "    title, misclassified_images, misclassified_true_labels, misclassified_pred_labels\n",
    "):\n",
    "    random.seed(random_seed)\n",
    "    misclassified_indices_sample = random.sample(\n",
    "        range(len(misclassified_images)),\n",
    "        min(num_misclassified_to_show, len(misclassified_images)),\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(15, 4))\n",
    "\n",
    "    for idx, misclassified_idx in enumerate(misclassified_indices_sample):\n",
    "        image = misclassified_images[misclassified_idx]\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        true_label = misclassified_true_labels[misclassified_idx]\n",
    "        predicted_label = misclassified_pred_labels[misclassified_idx]\n",
    "\n",
    "        plt.subplot(1, num_misclassified_to_show, idx + 1)\n",
    "        plt.imshow(image_rgb)\n",
    "        plt.title(f\"True: {true_label}\\nPred: {predicted_label}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(title, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_metrics(title, all_precision, all_recall, all_f1):\n",
    "    average_precision = np.mean(all_precision)\n",
    "    average_recall = np.mean(all_recall)\n",
    "    average_f1 = np.mean(all_f1)\n",
    "\n",
    "    print(title)\n",
    "    print(f\"Average Precision: {average_precision}\")\n",
    "    print(f\"Average Recall: {average_recall}\")\n",
    "    print(f\"Average F1 Score: {average_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fisher(\n",
    "    training_set_person_i, training_label_set_person_i, testing_set_person_i\n",
    "):\n",
    "    train_images_flat = np.array([p.flatten() for p in training_set_person_i])\n",
    "    test_images_flat = np.array([p.flatten() for p in testing_set_person_i])\n",
    "\n",
    "    # Avoid singularity issue in LDA and help generalize\n",
    "    pca = PCA(n_components=min(5, train_images_flat.shape[0] - 1))\n",
    "    train_pca = pca.fit_transform(train_images_flat)\n",
    "    test_pca = pca.transform(test_images_flat)\n",
    "\n",
    "    lda = LDA()\n",
    "    lda.fit(train_pca, training_label_set_person_i)\n",
    "\n",
    "    train_fisherfaces = lda.transform(train_pca)\n",
    "    test_fisherfaces = lda.transform(test_pca)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(train_fisherfaces, training_label_set_person_i)\n",
    "    test_predictions = knn.predict(test_fisherfaces)\n",
    "    return test_predictions\n",
    "\n",
    "\n",
    "def eval_wavelet_scattering(\n",
    "    training_set_person_i, training_label_set_person_i, testing_set_person_i\n",
    "):\n",
    "    train_scattered = np.array(\n",
    "        [scattering(img.astype(np.float32)) for img in training_set_person_i]\n",
    "    )\n",
    "    test_scattered = np.array(\n",
    "        [scattering(img.astype(np.float32)) for img in testing_set_person_i]\n",
    "    )\n",
    "\n",
    "    pca = PCA(n_components=5)\n",
    "    train_pca = pca.fit_transform(train_scattered.reshape(len(train_scattered), -1))\n",
    "    test_pca = pca.transform(test_scattered.reshape(len(test_scattered), -1))\n",
    "\n",
    "    svm = SVC(kernel=\"linear\")\n",
    "    svm.fit(train_pca, training_label_set_person_i)\n",
    "    test_predictions = svm.predict(test_pca)\n",
    "    return test_predictions\n",
    "\n",
    "\n",
    "def eval_bovw_kpsift_kpsift(\n",
    "    training_set_person_i, training_label_set_person_i, testing_set_person_i\n",
    "):\n",
    "    _, train_desc = extract_sift_features(training_set_person_i)\n",
    "    trained_svm, kmeans = train_bovw_svm(train_desc, training_label_set_person_i)\n",
    "    test_predictions = [\n",
    "        predict_with_svm(trained_svm, kmeans, xi) for xi in testing_set_person_i\n",
    "    ]\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ensemble(useSym):\n",
    "    all_precision = []\n",
    "    all_recall = []\n",
    "    all_f1 = []\n",
    "\n",
    "    misclassified_images = []\n",
    "    misclassified_true_labels = []\n",
    "    misclassified_pred_labels = []\n",
    "\n",
    "    for i in range(len(y_test)):\n",
    "        try:\n",
    "            training_label_set_person_i = y_train[i]\n",
    "            training_set_person_i, frontalized_training_set_person_i = normalize_list(\n",
    "                x_train[i], useSym\n",
    "            )\n",
    "\n",
    "            testing_label_set_person_i = y_test[i]\n",
    "            testing_set_person_i, frontalized_testing_set_person_i = normalize_list(\n",
    "                x_test[i], useSym\n",
    "            )\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                future_bowv = executor.submit(\n",
    "                    eval_bovw_kpsift_kpsift,\n",
    "                    training_set_person_i,\n",
    "                    training_label_set_person_i,\n",
    "                    testing_set_person_i,\n",
    "                )\n",
    "\n",
    "                future_fisher = executor.submit(\n",
    "                    eval_fisher,\n",
    "                    frontalized_training_set_person_i,\n",
    "                    training_label_set_person_i,\n",
    "                    frontalized_testing_set_person_i,\n",
    "                )\n",
    "\n",
    "                future_wavelet = executor.submit(\n",
    "                    eval_wavelet_scattering,\n",
    "                    frontalized_training_set_person_i,\n",
    "                    training_label_set_person_i,\n",
    "                    frontalized_testing_set_person_i,\n",
    "                )\n",
    "\n",
    "                bowv_kpsift_test_predictions = future_bowv.result()\n",
    "                fisher_test_predictions = future_fisher.result()\n",
    "                wavelet_scattering_test_predictions = future_wavelet.result()\n",
    "\n",
    "            all_predictions = np.vstack(\n",
    "                [\n",
    "                    fisher_test_predictions,\n",
    "                    bowv_kpsift_test_predictions,\n",
    "                    wavelet_scattering_test_predictions,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            test_predictions = np.array(\n",
    "                [\n",
    "                    Counter(all_predictions[:, i]).most_common(1)[0][0]\n",
    "                    for i in range(all_predictions.shape[1])\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            precision, recall, f1 = calculate_metrics(\n",
    "                testing_label_set_person_i, test_predictions\n",
    "            )\n",
    "\n",
    "            all_precision.append(precision)\n",
    "            all_recall.append(recall)\n",
    "            all_f1.append(f1)\n",
    "\n",
    "            (\n",
    "                misclassified_batch_images,\n",
    "                misclassified_batch_true_labels,\n",
    "                misclassified_batch_pred_labels,\n",
    "            ) = track_misclassifications(\n",
    "                testing_set_person_i,\n",
    "                # frontalized_testing_set_person_i,\n",
    "                testing_label_set_person_i,\n",
    "                test_predictions,\n",
    "            )\n",
    "\n",
    "            misclassified_images.extend(misclassified_batch_images)\n",
    "            misclassified_true_labels.extend(misclassified_batch_true_labels)\n",
    "            misclassified_pred_labels.extend(misclassified_batch_pred_labels)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"An error occurred at index {i} for one of the images: {e} Skipping {y_train[i][0]}'s dataset.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "    return (all_precision, all_recall, all_f1), (\n",
    "        misclassified_images,\n",
    "        misclassified_true_labels,\n",
    "        misclassified_pred_labels,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ensemble, misclassified_ensemble = eval_ensemble(useSym=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(\"Using Ensemble\", *metrics_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassifications(\"Using Ensemble\", *misclassified_ensemble)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
