{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Scattering Wavelets + FaceFrontalization\n",
    "\n",
    "Using an SVM and FCN as the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request as urlreq\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from kymatio.numpy import Scattering2D\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "import scipy.io as scio\n",
    "from face_frontalization import frontalize\n",
    "from face_frontalization import camera_calibration as calib\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 8\n",
    "\n",
    "J = 3\n",
    "min_image_num = 10\n",
    "training_set_size = 10\n",
    "training_image_num = 5\n",
    "num_misclassified_to_show = 5\n",
    "\n",
    "frontalize_face = False\n",
    "crop_face_after_norm = True\n",
    "face_image_target_size = (64, 64)\n",
    "\n",
    "base_folder = \"data/face/lfw-deepfunneled/lfw-deepfunneled\"\n",
    "haarcascade = \"model_checkpoints/haarcascade.xml\"\n",
    "haarcascade_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt2.xml\"\n",
    "\n",
    "LBFmodel = \"model_checkpoints/lbfmodel.yaml\"\n",
    "LBFmodel_url = \"https://github.com/kurnianggoro/GSOC2017/raw/master/data/lbfmodel.yaml\"\n",
    "\n",
    "frontalize_model_name = \"model_dlib\"\n",
    "frontalize_model_path = \"model_checkpoints/model3Ddlib.mat\"\n",
    "\n",
    "eye_mask_mat = \"eyemask\"\n",
    "eye_mask_mat_path = \"model_checkpoints/eyemask.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, url in zip([haarcascade, LBFmodel], [haarcascade_url, LBFmodel_url]):\n",
    "    if os.path.exists(filename):\n",
    "        print(\"File exists\")\n",
    "    else:\n",
    "        urlreq.urlretrieve(url, filename)\n",
    "        print(\"File downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for person in os.listdir(base_folder):\n",
    "    person_dir = os.path.join(base_folder, person)\n",
    "    if os.path.isdir(person_dir):\n",
    "        images = []\n",
    "        for img_name in os.listdir(person_dir):\n",
    "            img_path = os.path.join(person_dir, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            if not frontalize_face and not crop_face_after_norm:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "        if len(images) >= min_image_num:\n",
    "            data[person] = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "people_names = list(data.keys())\n",
    "\n",
    "for person, images in data.items():\n",
    "    train_images = []\n",
    "    test_images = []\n",
    "    random.seed(random_seed)\n",
    "    random.shuffle(images)\n",
    "\n",
    "    train_images.extend(images[:training_image_num])\n",
    "    test_images.extend(images[training_image_num:])\n",
    "    train_labels = [person] * training_image_num\n",
    "    test_labels = [person] * (len(images) - training_image_num)\n",
    "\n",
    "    random.seed(random_seed)\n",
    "    other_people = random.sample(\n",
    "        [p for p in people_names if p != person],\n",
    "        2 * (training_set_size - training_image_num),\n",
    "    )\n",
    "\n",
    "    for i, other_person in enumerate(other_people):\n",
    "        random.seed(random_seed)\n",
    "        chosen_image = random.sample(data[other_person], 1)[0]\n",
    "        if i % 2 == 0:\n",
    "            train_images.append(chosen_image)\n",
    "            train_labels.append(\"Unknown\")\n",
    "        else:\n",
    "            test_images.append(chosen_image)\n",
    "            test_labels.append(\"Unknown\")\n",
    "\n",
    "    x_train.append(train_images)\n",
    "    x_test.append(test_images)\n",
    "    y_train.append(train_labels)\n",
    "    y_test.append(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scattering = Scattering2D(\n",
    "    J=J,\n",
    "    shape=(\n",
    "        face_image_target_size\n",
    "        if crop_face_after_norm\n",
    "        else (250, 250)\n",
    "    ),\n",
    ")\n",
    "\n",
    "face_detector = cv2.CascadeClassifier(haarcascade)\n",
    "landmark_detector = cv2.face.createFacemarkLBF()\n",
    "landmark_detector.loadModel(LBFmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_face(img):\n",
    "    model3D = frontalize.ThreeD_Model(frontalize_model_path, frontalize_model_name)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    if len(faces) == 0:\n",
    "        raise RuntimeError(\"No faces detected.\")\n",
    "\n",
    "    main_face = np.array([max(faces, key=lambda rect: rect[2] * rect[3])])\n",
    "    retval, landmarks = landmark_detector.fit(gray, main_face)\n",
    "    if not retval or len(landmarks) == 0:\n",
    "        raise RuntimeError(\"Could not detect landmarks.\")\n",
    "\n",
    "    # OpenCV returns landmarks as a list, where each element is an array of shape (1, 68, 2).\n",
    "    lmarks = landmarks[0][0]\n",
    "    proj_matrix, _, _, _ = calib.estimate_camera(model3D, lmarks)\n",
    "\n",
    "    eyemask = np.asarray(scio.loadmat(eye_mask_mat_path)[eye_mask_mat])\n",
    "    frontal_raw, frontal_sym = frontalize.frontalize(\n",
    "        img, proj_matrix, model3D.ref_U, eyemask\n",
    "    )\n",
    "\n",
    "    return frontal_raw, frontal_sym\n",
    "\n",
    "\n",
    "def obtain_only_face(frontal_view):\n",
    "    faces = face_detector.detectMultiScale(\n",
    "        frontal_view, scaleFactor=1.1, minNeighbors=5\n",
    "    )\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        raise RuntimeError(\"No faces detected (after frontalization).\")\n",
    "\n",
    "    main_face = np.array([max(faces, key=lambda rect: rect[2] * rect[3])])\n",
    "    _, landmarks = landmark_detector.fit(frontal_view, main_face)\n",
    "\n",
    "    lmarks = landmarks[0][0]\n",
    "    hull = cv2.convexHull(np.array(lmarks, dtype=np.int32))\n",
    "\n",
    "    min_x = min(lmarks, key=lambda p: p[0])[0]\n",
    "    max_x = max(lmarks, key=lambda p: p[0])[0]\n",
    "    min_y = min(lmarks, key=lambda p: p[1])[1]\n",
    "    max_y = max(lmarks, key=lambda p: p[1])[1]\n",
    "\n",
    "    mask = np.zeros((frontal_view.shape[0], frontal_view.shape[1]), dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, [hull], 255)\n",
    "\n",
    "    masked_face = frontal_view.copy()\n",
    "    if masked_face.dtype != np.uint8:\n",
    "        masked_face = np.uint8(np.clip(masked_face, 0, 255))\n",
    "\n",
    "    masked_face[mask == 0] = 0\n",
    "    masked_face = masked_face[\n",
    "        int(min_y) - 5 : int(max_y) + 5, int(min_x) - 5 : int(max_x) + 5\n",
    "    ]\n",
    "\n",
    "    masked_face = cv2.cvtColor(masked_face, cv2.COLOR_BGR2GRAY)\n",
    "    resized_face = cv2.resize(masked_face, face_image_target_size)\n",
    "    return resized_face\n",
    "\n",
    "\n",
    "def normalize_list(image_list, useSym):\n",
    "    input = image_list\n",
    "    if frontalize_face:\n",
    "        input = [normalize_face(x)[1 if useSym else 0] for x in image_list]\n",
    "\n",
    "    if crop_face_after_norm:\n",
    "        return [obtain_only_face(x) for x in input]\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true_labels, predicted_labels):\n",
    "    precision = precision_score(\n",
    "        true_labels,\n",
    "        predicted_labels,\n",
    "        average=\"weighted\",\n",
    "        labels=np.unique(true_labels),\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    recall = recall_score(\n",
    "        true_labels,\n",
    "        predicted_labels,\n",
    "        average=\"weighted\",\n",
    "        labels=np.unique(true_labels),\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    return precision, recall, accuracy\n",
    "\n",
    "\n",
    "def track_misclassifications(test_images, true_labels, predicted_labels):\n",
    "    misclassified_images = []\n",
    "    misclassified_true_labels = []\n",
    "    misclassified_pred_labels = []\n",
    "\n",
    "    true_labels = np.array(true_labels)\n",
    "    misclassified_indices = np.where(predicted_labels != true_labels)[0]\n",
    "\n",
    "    for idx in misclassified_indices:\n",
    "        misclassified_images.append(test_images[idx])\n",
    "        misclassified_true_labels.append(true_labels[idx])\n",
    "        misclassified_pred_labels.append(predicted_labels[idx])\n",
    "\n",
    "    return misclassified_images, misclassified_true_labels, misclassified_pred_labels\n",
    "\n",
    "\n",
    "def visualize_misclassifications(\n",
    "    title, misclassified_images, misclassified_true_labels, misclassified_pred_labels\n",
    "):\n",
    "    random.seed(random_seed)\n",
    "    misclassified_indices_sample = random.sample(\n",
    "        range(len(misclassified_images)),\n",
    "        min(num_misclassified_to_show, len(misclassified_images)),\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(15, 4))\n",
    "\n",
    "    for idx, misclassified_idx in enumerate(misclassified_indices_sample):\n",
    "        image = misclassified_images[misclassified_idx]\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        true_label = misclassified_true_labels[misclassified_idx]\n",
    "        predicted_label = misclassified_pred_labels[misclassified_idx]\n",
    "\n",
    "        plt.subplot(1, num_misclassified_to_show, idx + 1)\n",
    "        plt.imshow(image_rgb)\n",
    "        plt.title(f\"True: {true_label}\\nPred: {predicted_label}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    title = f\"{title} ({f'With Face Frontalization and{\" without\" if not crop_face_after_norm else \"\"} Face Croping' if frontalize_face else 'Without Face Frontalization'})\"\n",
    "    plt.suptitle(title, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_metrics(title, all_precision, all_recall, all_accuracy):\n",
    "    average_precision = np.mean(all_precision)\n",
    "    average_recall = np.mean(all_recall)\n",
    "    average_accuracy = np.mean(all_accuracy)\n",
    "\n",
    "    title = f\"{title} ({f'With Face Frontalization and{\" without\" if not crop_face_after_norm else \"\"} Face Croping' if frontalize_face else 'Without Face Frontalization'})\"\n",
    "\n",
    "    print(title)\n",
    "    print(f\"Average Precision: {average_precision}\")\n",
    "    print(f\"Average Recall: {average_recall}\")\n",
    "    print(f\"Average Accuracy: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fcn(x_train, y_train):\n",
    "    train_pca_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    train_labels_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "    num_classes = 2\n",
    "    input_dim = x_train.shape[1]\n",
    "    model = DenseNet(input_dim, num_classes)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(10):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_pca_tensor)\n",
    "        loss = criterion(outputs, train_labels_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_fcn(model, x_test):\n",
    "    test_pca_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_pca_tensor)\n",
    "        test_predictions = torch.argmax(test_outputs, dim=1).numpy()\n",
    "\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_waveletscattering(useSym):\n",
    "    all_precision = []\n",
    "    all_recall = []\n",
    "    all_accuracy = []\n",
    "\n",
    "    misclassified_images = []\n",
    "    misclassified_true_labels = []\n",
    "    misclassified_pred_labels = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        try:\n",
    "            training_label_set_person_i = y_train[i]\n",
    "            training_set_person_i = normalize_list(x_train[i], useSym)\n",
    "\n",
    "            testing_label_set_person_i = y_test[i]\n",
    "            testing_set_person_i = normalize_list(x_test[i], useSym)\n",
    "\n",
    "            train_scattered = np.array(\n",
    "                [scattering(img.astype(np.float32)) for img in training_set_person_i]\n",
    "            )\n",
    "            test_scattered = np.array(\n",
    "                [scattering(img.astype(np.float32)) for img in testing_set_person_i]\n",
    "            )\n",
    "\n",
    "            pca = PCA(n_components=5)\n",
    "            train_pca = pca.fit_transform(\n",
    "                train_scattered.reshape(len(train_scattered), -1)\n",
    "            )\n",
    "            test_pca = pca.transform(test_scattered.reshape(len(test_scattered), -1))\n",
    "\n",
    "            svm = SVC(kernel=\"linear\")\n",
    "            svm.fit(train_pca, training_label_set_person_i)\n",
    "            test_predictions = svm.predict(test_pca)\n",
    "\n",
    "            precision, recall, accuracy = calculate_metrics(\n",
    "                testing_label_set_person_i, test_predictions\n",
    "            )\n",
    "\n",
    "            all_precision.append(precision)\n",
    "            all_recall.append(recall)\n",
    "            all_accuracy.append(accuracy)\n",
    "\n",
    "            (\n",
    "                misclassified_batch_images,\n",
    "                misclassified_batch_true_labels,\n",
    "                misclassified_batch_pred_labels,\n",
    "            ) = track_misclassifications(\n",
    "                testing_set_person_i,\n",
    "                testing_label_set_person_i,\n",
    "                test_predictions,\n",
    "            )\n",
    "\n",
    "            misclassified_images.extend(misclassified_batch_images)\n",
    "            misclassified_true_labels.extend(misclassified_batch_true_labels)\n",
    "            misclassified_pred_labels.extend(misclassified_batch_pred_labels)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"An error occurred at index {i} for one of the images: {e} Skipping {y_train[i][0]}'s dataset.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "    return (all_precision, all_recall, all_accuracy), (\n",
    "        misclassified_images,\n",
    "        misclassified_true_labels,\n",
    "        misclassified_pred_labels,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_waveletscattering_fcn(useSym):\n",
    "    all_precision = []\n",
    "    all_recall = []\n",
    "    all_accuracy = []\n",
    "\n",
    "    misclassified_images = []\n",
    "    misclassified_true_labels = []\n",
    "    misclassified_pred_labels = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        try:\n",
    "            training_label_set_person_i = [\n",
    "                0 if y == \"Unknown\" else 1 for y in y_train[i]\n",
    "            ]\n",
    "            training_set_person_i = normalize_list(x_train[i], useSym)\n",
    "\n",
    "            testing_label_set_person_i = [0 if y == \"Unknown\" else 1 for y in y_test[i]]\n",
    "            testing_set_person_i = normalize_list(x_test[i], useSym)\n",
    "\n",
    "            train_data = np.array(\n",
    "                [scattering(img.astype(np.float32)) for img in training_set_person_i]\n",
    "            ).reshape(len(training_set_person_i), -1)\n",
    "\n",
    "            test_data = np.array(\n",
    "                [scattering(img.astype(np.float32)) for img in testing_set_person_i]\n",
    "            ).reshape(len(testing_set_person_i), -1)\n",
    "\n",
    "            pca = PCA(n_components=5)\n",
    "            train_data = pca.fit_transform(train_data)\n",
    "            test_data = pca.transform(test_data)\n",
    "\n",
    "            model = train_fcn(train_data, training_label_set_person_i)\n",
    "            test_predictions = test_fcn(model, test_data)\n",
    "\n",
    "            precision, recall, accuracy = calculate_metrics(\n",
    "                testing_label_set_person_i, test_predictions\n",
    "            )\n",
    "\n",
    "            all_precision.append(precision)\n",
    "            all_recall.append(recall)\n",
    "            all_accuracy.append(accuracy)\n",
    "\n",
    "            (\n",
    "                misclassified_batch_images,\n",
    "                misclassified_batch_true_labels,\n",
    "                misclassified_batch_pred_labels,\n",
    "            ) = track_misclassifications(\n",
    "                testing_set_person_i,\n",
    "                testing_label_set_person_i,\n",
    "                test_predictions,\n",
    "            )\n",
    "\n",
    "            misclassified_images.extend(misclassified_batch_images)\n",
    "            misclassified_true_labels.extend(misclassified_batch_true_labels)\n",
    "            misclassified_pred_labels.extend(misclassified_batch_pred_labels)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"An error occurred at index {i} for one of the images: {e} Skipping {y_train[i][0]}'s dataset.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "    return (all_precision, all_recall, all_accuracy), (\n",
    "        misclassified_images,\n",
    "        misclassified_true_labels,\n",
    "        misclassified_pred_labels,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_scattering, misclassified_scattering = eval_waveletscattering(useSym=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(\"Using Wavelet Scattering\", *metrics_scattering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassifications(\"Using Wavelet Scattering\", *misclassified_scattering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_scattering_fcn, misclassified_scattering_fcn = eval_waveletscattering_fcn(\n",
    "    useSym=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(\"Using Wavelet Scattering FCN\", *metrics_scattering_fcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_misclassifications(\n",
    "    \"Using Wavelet Scattering FCN\", *misclassified_scattering_fcn\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
